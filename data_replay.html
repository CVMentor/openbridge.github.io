<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Openbridge.github.io by openbridge</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Openbridge Developer Documentation</h1>
        <p class="view">Visit us @ <a href="http://www.openbridge.com">www.openbridge.com</a> or if you need support site head over to <a href="openbridge.zendesk.com">openbridge.zendesk.com </a></p>
     
      </header>

      <section>
<h3 class="view"><a href="index.html">&lt;-Back to Home</a></h3>
<p><small>Last update: 12-02-2014</small></p><strong>Version: v1.0</strong>
       <h1>Openbridge Data Replay Services</h1>
       <h2>Overview</h2>

<p>In addition to storage used within the Data Warehouse and its associated backups, Openbridge archives all transactional data sent to our services. This includes the “raw” data sent to either the Streaming API or to Stacks.  There are two parts to Replay Services; real-time in-memory deduplication and long term archives.

<h2>Real-time Data Deduplication Cache</h2>
<p>In an effort to increase data quality, every message, record, event, etc.  delivered to Openbridge, via Streaming API or Stacks, is uniquely hashed. That hash is then stored within an in-memory key-value store. Every subsequent message, record or event hash is evaluated in real-time to see if the hash already exists. If a duplicate hash is found, the system will flag it and not allow further processing or storage into the database. It will be set aside that data into a “duplicate” vault and made available for further analytics. This real-time “pre-processing” of data in-flight significantly reduces the risk of having duplicate records in downstream data warehouses, especially for log oriented data originating from web analytics tools.

<p>All data within the cache is “synced” to disk every second. The synced representation of the in-memory state is further backed up every hour for 48 hours with daily snapshots occurring for a month.

<h3>Shared Cache</h3>
<p>All accounts include access to a shared cache. The typically time-to-live (TTL) for hash in the shared cache is approximately 24-48 hours.  Dedicated caches are offered. More details on that below

<h3>Dedicated Cache (Optional)</h3>
<p>There are cases where a customer may want a longer TTL for their hashes or simply want a resource dedicated for their account. Customers can purchase dedicated capacity which align with expected volumes and a desired TTL. About 34 million  hashes can be stored per gigabyte of ram cache. 
The following are the available instance types and the estimated number of hash messages that can be stored:


<table width="500" border="0">
  <tr>
    <td><strong>Type</strong></td>
    <td><strong>CPU Cores</strong></td>
    <td><strong>Memory<strong></td>
    <td><strong>Storage Per Node</strong></td>
    <td><strong>Max Records</strong></td>
  </tr>
  <tr>
    <td>SMALL</td>
    <td>2</td>
    <td>15GB</td>
    <td>32GB SSD</td>
    <td>510 million</td>
  </tr>
  <tr>
    <td>MEDIUM</td>
    <td>4</td>
    <td>30.5</td>
    <td>80 GB SSD</td>
    <td>1 Billion</td>
  </tr>
  <tr>
    <td>LARGE</td>
    <td>8</td>
    <td>61</td>
    <td>160 GB SSD</td>
    <td>2 billion</td>
  </tr>
  <tr>
     <td>XL</td>
    <td>16</td>
    <td>122GB</td>
    <td>320 GB SSD</td>
    <td>4 billion</td>
  </tr>
</table>

<p>It is important to note that the the actual velocity and volume of data will determine the approximate time a message/record will be retained in the cache. For example, if you are processing 25 million messages a day and have deployed a medium cache you will be able to persist approximately 5 weeks worth of data in your cache.  If you process 10M messages a day and want to persist your data for up to 1 year, you would deploy an XL node.


 <h2>Long Term Storage Archives</h2> 
<p>Openbridge will keep transactional logs of data returned from any Data Bridge service we manage on your behalf.  This data is kept on “ice” in the event there is a need to “replay” the original data sent into Openbridge for auditing, restores or replacement. We keep this data for up to 30 days.  This provides secure and durable storage for data archiving and backup of snapshots original source. All raw data is sent in transit over Secure Sockets Layer (SSL) and automatically encrypted at rest using Advanced Encryption Standard (AES) 256-bit symmetric keys.  
  
  
<h2>Notes</h2>
<p>Pricing is based on the specific system sizing requirements.
<p>By default, we retain transactional archives for 30 days for any data coming through the Data Pipeline and Bridge services. You can request the retention period to be as long as you need. If you would like longer backup windows beyond what is included there may be incremental costs.


      </section>
      <footer>
        <p><small>Hosted on GitHub Pages</small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>

  </body>
</html>
